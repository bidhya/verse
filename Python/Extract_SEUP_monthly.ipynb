{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "487981c6-da6a-4c0d-829a-b8a37c9653a4",
   "metadata": {},
   "source": [
    "# Process SEUP NETCDF data [Run this only on  NASA Discover HPC]\n",
    "1. Generate Daily Data by converting 3-hourly SEUP data\n",
    "2. Concatenate monthly files to get water year file\n",
    "3. Combine individual WY nc files files into one file with all varabiles\n",
    "\n",
    "# TODO\n",
    "- check why this is not exactly 0.05 but rather DX=0.05000000074505806; same issue for DY  \n",
    "-- for now fix is to hard-code both DX and DY = 0.05 explicitly\n",
    "\n",
    "\n",
    "Sep 30, 2022: Updated to change mm to m for snowfall and swe  \n",
    "Feb 21, 2023\n",
    "\n",
    "Info: ~ 2.2 million (2,198,176) pixels for North America at 0.05 degree lat/lon\n",
    "Time resolution : each file every 3 hours [00, 03, 06, 09, 12, 15, 18, 21 hours]  \n",
    "in_vars = ['MODSCAG', 'WRFG', 'WRFP', 'WRFSWE', 'WRFT']  \n",
    "\n",
    "## List of needed variables\n",
    "### Based on list here.\n",
    "- Snowfall: Snowf : for now\n",
    "- Rainfall: Rainf  (for future)\n",
    "- SWE: SWE\n",
    "- Air temperature: Tair_f\n",
    "- Ground heat flux: Qg\n",
    "\n",
    "\n",
    "## Issues\n",
    "- longitude and latitude are both 2-D arrays and embedded as variable. They should each be 1-D coords (and dims)  \n",
    "    - probably easier to construct lat/lon manually rather than extracting from 2-D array (but do verify at some point the values are correct)   \n",
    "- Extract datetime from filename itself\n",
    "- there is difference between groupby and resample operations; though data remains same (for sum) or similar (for mean)  \n",
    "## Outstanding\n",
    "- background/no-data/masking area outside continent\n",
    "- cf-compliance? (nice to have)  \n",
    "\n",
    "## Conceptual\n",
    "~~- Missing data inland (perhaps over wate bodies).~~  \n",
    "~~- How to distinguish between inland missing and area outside continent (does not matter perhaps)~~\n",
    "\n",
    "## Outputs\n",
    "1. Monthly nc files\n",
    "2. Water Year nc files\n",
    "3. Water Year all variables combined into one file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2768cbb-feb5-4f0a-afc4-42be200e1298",
   "metadata": {},
   "source": [
    "### 0: Snowf_tavg  \n",
    "units : kg m-2 s-1  \n",
    "standard_name : snowfall_rate  \n",
    "long_name : snowfall rate  \n",
    "\n",
    "~~### 1 : Rainf_tavg  \n",
    "units : kg m-2 s-1  \n",
    "standard_name : precipitation_rate  \n",
    "long_name : precipitation rate~~  \n",
    "\n",
    "### 2: SWE_tavg  :: for daily, statistics = max\n",
    "units : kg m-2  \n",
    "standard_name : liquid_water_content_of_surface_snow  \n",
    "long_name : snowfall rate  \n",
    "\n",
    "### 3: Tair_f_tavg  : for daily, statistics = mean  \n",
    "units :  K  \n",
    "standard_name : air_temperature  \n",
    "long_name : air temperature\n",
    "\n",
    "### 4: Qg_tavg  : for daily, statistics = sum?  should be mean  [july 29th]  \n",
    "units : W m-2  \n",
    "standard_name :  downward_heat_flux_in_soil  \n",
    "long_name :  soil heat flux  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109b1be-5eaf-4b2f-988a-b269f301e530",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is parameters passed from outside\n",
    "cores = 10  # hardcoded to -1 below, so this has no effect currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf24d0-daa1-4f51-ac25-35d445f17e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import platform\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# node is called host_machine in Julia\n",
    "# ss, node, = platform.uname()[:2]\n",
    "ss = platform.system()\n",
    "node = platform.node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba8721-b2dd-4ca0-b2ec-253d52fd2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'STAFF-BY-M01' in node and \"Windows\" in ss:\n",
    "    root_dir = \"C:\"\n",
    "    base_folder = f\"{root_dir}/Github/coressd/Blender/NoahMP\"\n",
    "    in_folder = f\"{base_folder}/3hourly\"\n",
    "    out_folder = f\"{base_folder}/temp\"\n",
    "elif 'STAFF-BY-M01' in node and \"Linux\" in ss:\n",
    "    root_dir = \"/mnt/c\"  #\"/home/yadav.111\"\n",
    "    base_folder = f\"{root_dir}/Github/coressd/Blender/NoahMP\"\n",
    "    in_folder = f\"{base_folder}/3hourly\"\n",
    "    out_folder = f\"{base_folder}/temp\"\n",
    "elif \"borg\" in node and \"Linux\" in ss:\n",
    "    # nasa-discover nodes\n",
    "    base_folder = \"/discover/nobackup/projects/SEUP/mwrzesie/KEEP_fromRhaeSung/SEUP/NoahMP/MERRA2/OL/OUTPUT/SURFACEMODEL\"\n",
    "    in_folder = base_folder\n",
    "    out_folder = \"/discover/nobackup/projects/coressd/Blender/NoahMP\"\n",
    "else:\n",
    "    print(\"Unknow system. in_folder and out_folder cannot be set\")\n",
    "    assert(False)\n",
    "print(f\"in_folder  : {in_folder}\")\n",
    "print(f\"out_folder : {out_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ef198-490c-451c-aa2f-760b8d455ac3",
   "metadata": {},
   "source": [
    "# 1. Generate Daily Data\n",
    "By concatenating 3-hourly datasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d04c2-bbc3-4261-b453-3b768cc035c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yr_mon_list = os.listdir(f\"{base_folder}/3hourly\")\n",
    "# or, Explicitly specify year month list to process\n",
    "yr_mon_list = ['201510', '201511', '201512', '201601', '201602', '201603', '201604', '201605', '201606', '201607', '201608', '201609']\n",
    "variables = [\"Snowf_tavg\", \"SWE_tavg\", \"Tair_f_tavg\", \"Qg_tavg\"]  #\"Rainf_tavg\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366a9d6-f018-4945-b843-dd918bc156e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_seup(in_folder, month_subfolder, out_folder):\n",
    "    \"\"\" Combine 3-hourly SEUP data to daily\n",
    "    Need to pass these variables\n",
    "    combined_folder = f'{out_folder}/combined/{var}'\n",
    "    combined_nc_file = f'{combined_folder}/{month_subfolder}.nc' #appended monthly to indicate this is monthly mean data\n",
    "    nc_files\n",
    "    f\"{folder}/{nc_file}\"\n",
    "\n",
    "    \"\"\"\n",
    "    folder = f\"{in_folder}/{month_subfolder}\"\n",
    "    nc_files = [f for f in os.listdir(folder) if f.endswith(\".nc\")]\n",
    "    # Sort in ascending order of data\n",
    "    nc_files.sort(key = lambda x: pd.to_datetime(x.split(\"_\")[2].split(\".\")[0]))\n",
    "    for var in variables:\n",
    "        print(f\"\\tVariable: {var}\")\n",
    "        dt_list = []\n",
    "        # We will save the combined netcdf file here\n",
    "        combined_folder = f'{out_folder}/combined/{var}'\n",
    "        # if not os.path.exists(combined_folder):  # creates problem with joblib parallization, perhaps due to two threads tyring to create same directory\n",
    "        os.makedirs(combined_folder, exist_ok=True)  # TODO: better to create this folders outside of this function to avoid joblib error\n",
    "        combined_nc_file = f'{combined_folder}/{month_subfolder}.nc' #appended monthly to indicate this is monthly mean data\n",
    "        if not os.path.exists(combined_nc_file):\n",
    "            # Combine 3-hourly data for one varialbe into 1 file\n",
    "            count = 0\n",
    "            for nc_file in nc_files:\n",
    "                # print(f\"nc_file : {nc_file}\")\n",
    "                dt_list.append(pd.to_datetime(nc_file.split(\"_\")[2].split(\".\")[0]))\n",
    "                if count == 0:\n",
    "                    ds = xr.open_dataset(f\"{folder}/{nc_file}\", engine='netcdf4')\n",
    "                    da = ds[var]\n",
    "                else:\n",
    "                    ds = xr.open_dataset(f\"{folder}/{nc_file}\", engine='netcdf4')\n",
    "                    da_temp = ds[var]\n",
    "                    da = xr.concat([da, da_temp], dim='time')  #, dim='time'\n",
    "                count += 1\n",
    "\n",
    "            # Get (Assign) lat/lon\n",
    "            # ds[\"X\"] = np.int64(ds.X)\n",
    "            # missing_value = ds.missing_value\n",
    "            ll_lon = ds.attrs[\"SOUTH_WEST_CORNER_LON\"]\n",
    "            ll_lat = ds.attrs[\"SOUTH_WEST_CORNER_LAT\"]\n",
    "            # Hardcode DX and DY because extracting this from attrs adds some decimals (probably float related or real not sure)\n",
    "            DX = 0.05 # ds.attrs[\"DX\"]  #grid spacing in x-direction; TODO: check why this is not exactly 0.05 but rather DX=0.05000000074505806; same issue for DY\n",
    "            DY = 0.05 #ds.attrs[\"DY\"]  #grid spacing in y-direction; \n",
    "            print(f\"ll_lon: {ll_lon} , ll_lat: {ll_lat}, DX: {DX}, DY: {DY}\")\n",
    "            # Construct lon and lat coordiantes\n",
    "            lon = ds.east_west*DX + ll_lon\n",
    "            lat = ds.north_south*DY + ll_lat\n",
    "            # Coordinates can also be set or removed by using the dictionary like syntax:\n",
    "            da[\"x\"] = lon\n",
    "            da[\"y\"] = lat\n",
    "            da[\"time\"] = dt_list ## better?\n",
    "            da = da.rename({\"east_west\":\"x\", \"north_south\":\"y\"})  #dims and coordinates name should match; new warning as of 10/25/2022\n",
    "            print(\"\\nResampling to Daily\\n\")\n",
    "            if var in [\"Snowf_tavg\", \"Rainf_tavg\"]:\n",
    "                print(var)\n",
    "                # Converr the accumalation rate per second to per hour; then how much it accumulated in 3 hours\n",
    "                da = da*3600*3  #units : kg m-2 s-1   to kg m-2 hr-1 convert rate to accumulation per second in 3 hours;ie, 3600 seconds x 3 hours\n",
    "                daily = da.resample(time=\"1D\").sum(skipna=False)  # or 24H; retains actual datetime\n",
    "                daily.data = daily.data/1000.0  # convert from mm to meter\n",
    "            elif var == \"SWE_tavg\":\n",
    "                print(var)\n",
    "                daily = da.resample(time=\"1D\").max(skipna=False)  # or 24H; retains actual datetime \n",
    "                daily.data = daily.data/1000.0  # convert from mm to meter\n",
    "            elif var == \"Tair_f_tavg\":\n",
    "                print(var)\n",
    "                daily = da.resample(time=\"1D\").mean(skipna=False)\n",
    "\n",
    "            elif var == \"Qg_tavg\":\n",
    "                print(var)\n",
    "                daily = da.resample(time=\"1D\").mean(skipna=False)\n",
    "            else:\n",
    "                print(\"Error: Unknown Varialbe. Stopping processing\")\n",
    "                assert(False)\n",
    "            # print(np.array_equal(daily.data, daily_g.data))  # at least data are equal when using resample and groupby\n",
    "            # # Conver to Dataset, so this is proper nc file\n",
    "            daily = xr.Dataset({var:daily})\n",
    "            daily.to_netcdf(combined_nc_file)  #260 MB size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277ecf7-4ad5-439d-a9d9-9538849f1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To procecess sequentially (in series)\n",
    "# for month_subfolder in yr_mon_list[1:3]:\n",
    "#     combine_seup(in_folder, month_subfolder, out_folder)\n",
    "# Or in parallel \n",
    "# (replace -1 by cores, but -1 seems better option here as we don't have to worry about explicity passing cores; will be decided on by nunber of cores asked in slurm scheduler\n",
    "Parallel(n_jobs=-1)(delayed(combine_seup) (in_folder, month_subfolder, out_folder) for month_subfolder in yr_mon_list[5:7])\n",
    "\n",
    "# release memory; should work whether these variables exist or not\n",
    "ds = None\n",
    "da = None\n",
    "da_temp = None\n",
    "daily = None  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f9f25-1a48-45a3-aca7-18e8f4731783",
   "metadata": {},
   "source": [
    "# 2. Concatenate monthly files to get water year file\n",
    "Independent of above code\n",
    "Here, each variable is in sepate folder/file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7580b62-4f84-470a-9783-c38a026cb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find way to manually generate this year_month file pre-fixes and in ascending order; \n",
    "# that way we do not need to depend on the way the monthly files are staged\n",
    "# os.listdir(f\"{out_folder}/combined/Snowf_tavg\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e772adaf-4bc5-41b0-bf2a-47cd5e99a478",
   "metadata": {},
   "source": [
    "# Using list comprehension takes way too long time perhaps due to memory issues\n",
    "ds_wy = xr.concat([xr.open_dataset(f\"{base_folder}/combined/{var}/{f}\", chunks=True) for f in monthly_nc_files], dim=\"time\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb910a-7e8b-4480-8369-5f9e4f20b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2016\"  # water year 2016 is from Oct 2015 to Sept 2016 \n",
    "for var in variables:\n",
    "    print(var)\n",
    "    # Get monthly input files\n",
    "    monthly_nc_files = os.listdir(f\"{out_folder}/combined/{var}\")  # probably need to generate the year month string from 201510 to 201609\n",
    "    wy_folder = f'{out_folder}/WY/{var}'\n",
    "    # if not os.path.exists(wy_folder):\n",
    "    os.makedirs(wy_folder, exist_ok=True)\n",
    "    combined_nc_file = f'{wy_folder}/{year}.nc' #appended monthly to indicate this is monthly mean data\n",
    "    # if not os.path.exists(combined_nc_file):  # need to create this regardless if file in combined folder is changed\n",
    "    f = monthly_nc_files[0]\n",
    "    wy_ds = xr.open_dataset(f\"{out_folder}/combined/{var}/{f}\", engine='netcdf4')\n",
    "    for f in monthly_nc_files[1:]:\n",
    "        ds_temp = xr.open_dataset(f\"{out_folder}/combined/{var}/{f}\", engine='netcdf4')  #, chunks={'time': 1}\n",
    "        # da_temp = ds[var]\n",
    "        wy_ds = xr.concat([wy_ds, ds_temp], dim='time')\n",
    "    wy_ds.to_netcdf(combined_nc_file)  # ~ 3 GB size\n",
    "wy_ds = None  # To prevent: Permission denied: b'C:\\\\Github\\\\coressd\\\\Blender\\\\NoahMP\\\\temp\\\\WY\\\\Snowf_tavg\\\\2016.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906e4fb8-f615-4ac4-89b3-4d6224bdf1e2",
   "metadata": {},
   "source": [
    "# 3. Combine individual WY nc files files into one file with all varabiles\n",
    "This is even bigger ~12 GB file\n",
    "Combine variables from different folder (STEP 2) into single file with all four variables for that water year  \n",
    "But may not be necessary ...  \n",
    "variables = ['Snowf_tavg', 'SWE_tavg', 'Tair_f_tavg', 'Qg_tavg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008e131-578b-4975-aedd-b33d9411fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2016\"  # water year 2016 is from Oct 2015 to Sept 2016 \n",
    "# ds_merged = xr.merge([xr.open_dataset(f\"{base_folder}/combined/{var}/{f}\", chunks=True) for f in monthly_nc_files])  #chunks={'time': 1} # use only for different variables\n",
    "wy_ds_merged = xr.merge([xr.open_dataset(f\"{out_folder}/WY/{v}/{year}.nc\") for v in variables])\n",
    "# Here merged implies merged on variables (in Xarray paralay)\n",
    "merged_wy_folder = f'{out_folder}/WY_merged'\n",
    "# if not os.path.exists(merged_wy_folder):\n",
    "os.makedirs(merged_wy_folder, exist_ok=True)\n",
    "merged_nc_file = f'{merged_wy_folder}/{year}_seup.nc' # this is later combined with MODIS-CGF for Blender Run\n",
    "# if not os.path.exists(merged_nc_file):\n",
    "wy_ds_merged.to_netcdf(merged_nc_file)  #12.6 GB size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88505a72-9cf4-411c-9174-54044eaa3b68",
   "metadata": {},
   "source": [
    "# END\n",
    "End of processing SEUP data for whole of north America for one Water Year  \n",
    "\n",
    "---  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8255f2d82bf71ce4305e4f309435784f802a480fe60b5ce467b8393b9d372257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
